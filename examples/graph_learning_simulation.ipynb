{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import generators as gen\n",
    "import optimize as opt\n",
    "import helpers as hel\n",
    "import pickle as pkl\n",
    "from NNet import NNet\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate graphs\n",
    "Create stochastic block models with 30 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiple_graphs(n_graphs=20, **kwargs):\n",
    "    return [gen.generate_L_sbm(seed=i, **kwargs) for i in range(n_graphs)]\n",
    "\n",
    "def to_pickle(obj, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pkl.dump(obj, f)\n",
    "        \n",
    "def from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ls_30 = create_multiple_graphs(20, nnodes=45, p_in=.3, p_out=.1, n_blocks=3)\n",
    "to_pickle(Ls_30, 'res/imp/Ls_30.pkl')\n",
    "\n",
    "Ls_50 = create_multiple_graphs(20, nnodes=45, p_in=.5, p_out=.1, n_blocks=3)\n",
    "to_pickle(Ls_50, 'res/imp/Ls_50.pkl')\n",
    "\n",
    "Ls_70 = create_multiple_graphs(20, nnodes=45, p_in=.7, p_out=.1, n_blocks=3)\n",
    "to_pickle(Ls_70, 'res/imp/Ls_70.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_impute(L, ker, imputer, n_samples=500):\n",
    "    \"\"\"Reproducibly generates 500 samples from L using kernel ker, then \n",
    "    imputes a graph using the imputer. Imputer is a function that\n",
    "    takes samples and returns an adjacency matrix.\"\"\"\n",
    "    \n",
    "    samples = gen.gen_and_filter(L, n_samples, ker=ker, seed=42)\n",
    "    return imputer(samples)\n",
    "\n",
    "\n",
    "def simulate(Ls, ker, imputer, verbose=True):\n",
    "    \"\"\"For every L in Ls, generates samples with ker and imputes the graph\n",
    "    using the imputer method. Imputer is a function that\n",
    "    takes samples and returns an adjacency matrix.\"\"\"\n",
    "    out = []\n",
    "    for i, L in enumerate(Ls):\n",
    "        if verbose:\n",
    "            print('\\rSimulating example {}'.format(i+1), end=' ')\n",
    "        out.append(sample_and_impute(L, ker, imputer))\n",
    "    return out\n",
    "\n",
    "def f1_scores(Ls, imps):\n",
    "    \"\"\"Calculates f1 scores between lists\"\"\"\n",
    "    \n",
    "    return [f1_score(np.asarray(L<0).flatten(), imp.flatten()) for L, imp in zip(Ls, imps)]\n",
    "\n",
    "def process_all(imputer, savefile_prefix):\n",
    "    kerlist = [gen.kernel_heat, gen.kernel_normal, \n",
    "               lambda x: gen.kernel_highpass(x, par=.5)]\n",
    "    kernames = ['heat', 'norm', 'high']\n",
    "    \n",
    "    Lslist = [Ls_30, Ls_50, Ls_70]\n",
    "    Lsnames = ['30', '50', '70']\n",
    "    \n",
    "    for ker, kername in zip(kerlist, kernames):\n",
    "        print('Kernel:', kername)\n",
    "        \n",
    "        for Ls, Lsname in zip(Lslist, Lsnames):\n",
    "            print('Ls:', Lsname)\n",
    "            \n",
    "            imps = simulate(Ls, ker, imputer)\n",
    "            f1 = f1_scores(Ls, imps)\n",
    "\n",
    "            savefile_suffix = '_' + Lsname + '_' + kername + '.pkl'\n",
    "            to_pickle(imps, savefile_prefix + savefile_suffix)\n",
    "            to_pickle(f1, savefile_prefix + '_f1' + savefile_suffix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Ls_30' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1bc3026f3c18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLs_30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Ls_30' is not defined"
     ]
    }
   ],
   "source": [
    "Ls+30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_imputer = lambda x: opt.impute_graph(x, lr=.01, verbose=False,\n",
    "                                         n_epochs=3000, lr_nnet=1e-3, nit_nnet=3)[0]>.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: heat\n",
      "Ls: 30\n",
      "Simulating example 1 "
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    process_all(our_imputer, 'res/imp/ours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_f1(file_prefix):\n",
    "    kernames = ['heat', 'norm', 'high']\n",
    "    Lsnames = ['30', '50', '70']\n",
    "    out = dict()\n",
    "    for kn in kernames:\n",
    "        out[kn] = dict()\n",
    "        for ln in Lsnames:\n",
    "            suffix = '_' + ln + '_' + kn + '.pkl'\n",
    "            out[kn][ln] = from_pickle(file_prefix + '_f1' + suffix)\n",
    "            \n",
    "    return out\n",
    "\n",
    "def get_iqr(x):\n",
    "    return np.subtract(*np.percentile(x, [75, 25]))\n",
    "\n",
    "def get_stats(f1_dict):\n",
    "    outlist = []\n",
    "    for ker, kerdict in f1_dict.items():\n",
    "        for pin, l in kerdict.items():\n",
    "            nparr = np.array(l)\n",
    "            outlist.append([ker, pin, nparr.mean(), np.median(nparr), \n",
    "                        nparr.std(), get_iqr(nparr)])\n",
    "    return pd.DataFrame(outlist, columns=['Filter', 'P_in', 'Mean', 'Median', 'Stdev', 'IQR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_f1 = load_f1('res/imp/ours')\n",
    "our_stats = get_stats(our_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_f1['norm']['70']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}